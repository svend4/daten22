"""
Document Indexer for MBTI RAG System
Loads, chunks, and indexes all documentation into ChromaDB
"""
import sys
from pathlib import Path
from typing import List
from tqdm import tqdm

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

from rag.config import (
    DOCS_DIR, TYPES_DIR, CHROMA_DIR, COLLECTION_NAME,
    CHUNK_SIZE, CHUNK_OVERLAP, EMBEDDING_MODEL
)


class MBTIDocumentIndexer:
    """Indexes MBTI documentation into vector database"""

    def __init__(self):
        print("ðŸš€ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€Ð° MBTI Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸...")

        # Initialize embeddings
        print(f"ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° embedding Ð¼Ð¾Ð´ÐµÐ»Ð¸: {EMBEDDING_MODEL}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

        self.documents = []
        self.chunks = []

    def load_documents(self) -> List[Document]:
        """Load all markdown documents from docs and types directories"""
        print("\nðŸ“š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²...")

        all_docs = []

        # Load from docs directory
        if DOCS_DIR.exists():
            print(f"  ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð· {DOCS_DIR}")
            loader = DirectoryLoader(
                str(DOCS_DIR),
                glob="**/*.md",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            docs = loader.load()
            print(f"    âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(docs)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            all_docs.extend(docs)

        # Load from types directory
        if TYPES_DIR.exists():
            print(f"  ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð· {TYPES_DIR}")
            loader = DirectoryLoader(
                str(TYPES_DIR),
                glob="**/*.md",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            docs = loader.load()
            print(f"    âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(docs)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            all_docs.extend(docs)

        # Add metadata
        for doc in all_docs:
            file_path = Path(doc.metadata['source'])
            doc.metadata['filename'] = file_path.name
            doc.metadata['directory'] = file_path.parent.name

            # Extract title from first line if it's a header
            lines = doc.page_content.split('\n')
            if lines and lines[0].startswith('#'):
                doc.metadata['title'] = lines[0].strip('#').strip()

        self.documents = all_docs
        print(f"\nâœ… Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾: {len(all_docs)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
        return all_docs

    def chunk_documents(self) -> List[Document]:
        """Split documents into chunks"""
        print("\nðŸ”ª Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹...")

        chunks = []
        for doc in tqdm(self.documents, desc="Chunking"):
            doc_chunks = self.text_splitter.split_documents([doc])
            chunks.extend(doc_chunks)

        self.chunks = chunks
        print(f"âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {len(chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
        return chunks

    def create_vectorstore(self) -> Chroma:
        """Create and populate vector store"""
        print("\nðŸ” Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        print(f"  ðŸ“ Ð›Ð¾ÐºÐ°Ñ†Ð¸Ñ: {CHROMA_DIR}")
        print(f"  ðŸ“¦ ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ: {COLLECTION_NAME}")

        # Create vector store
        vectorstore = Chroma.from_documents(
            documents=self.chunks,
            embedding=self.embeddings,
            collection_name=COLLECTION_NAME,
            persist_directory=str(CHROMA_DIR)
        )

        print("âœ… Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð±Ð°Ð·Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð°")
        return vectorstore

    def index_all(self):
        """Complete indexing pipeline"""
        print("=" * 60)
        print("ðŸŽ¯ Ð˜ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð˜Ð¯ Ð”ÐžÐšÐ£ÐœÐ•ÐÐ¢ÐÐ¦Ð˜Ð˜ MBTI")
        print("=" * 60)

        # Load documents
        self.load_documents()

        if not self.documents:
            print("âŒ Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹!")
            return

        # Chunk documents
        self.chunk_documents()

        # Create vector store
        vectorstore = self.create_vectorstore()

        # Print statistics
        print("\n" + "=" * 60)
        print("ðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ Ð˜ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð˜Ð˜")
        print("=" * 60)
        print(f"Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(self.documents)}")
        print(f"Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {len(self.chunks)}")
        print(f"Ð Ð°Ð·Ð¼ÐµÑ€ Ñ‡Ð°Ð½ÐºÐ°: {CHUNK_SIZE} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        print(f"ÐŸÐµÑ€ÐµÐºÑ€Ñ‹Ñ‚Ð¸Ðµ: {CHUNK_OVERLAP} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        print(f"Embedding Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {EMBEDDING_MODEL}")
        print(f"Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð‘Ð”: {CHROMA_DIR}")
        print("=" * 60)
        print("\nâœ¨ Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")

        return vectorstore

    def get_stats(self) -> dict:
        """Get indexing statistics"""
        return {
            'total_documents': len(self.documents),
            'total_chunks': len(self.chunks),
            'chunk_size': CHUNK_SIZE,
            'chunk_overlap': CHUNK_OVERLAP,
            'embedding_model': EMBEDDING_MODEL
        }


def main():
    """Main indexing function"""
    indexer = MBTIDocumentIndexer()
    indexer.index_all()


if __name__ == "__main__":
    main()
